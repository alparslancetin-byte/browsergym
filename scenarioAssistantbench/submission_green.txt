Q7.1 Environment Design
Describe how the environment is set up. For example, what is the goal of the task? What is the state space of the task environment? What are the actions a white agent can take in the environment? How does the state of the environment change when an white agent takes different actions? When is the task accomplished or ended?


Browsergym includes multiple different environments in one big package. We have implemented the following environments: MiniWoB,WebLINX and AssistantBench.

MiniWoB
Goal of the Task: The main goal for the agent in MiniWoB is to complete a variety of short, interactive web tasks presented through small HTML pages. Each task specifies a clear objective such as “Click the red button,” “Choose Iceland from the list,” or “Fill in the text field with ‘hello.’” The benchmark evaluates the agent’s ability to perceive visual and textual elements on a web page, interpret the given instruction, and perform the correct sequence of actions efficiently.
State Space: The state space in MiniWoB is defined by the HTML structure and rendered visual appearance of each task’s web page. At any point, the state includes the Web elements, their attributes (text, color, position, visibility), and the current page layout as perceived by the agent. Since each MiniWoB task is self-contained, the state resets when we start a new task.
Agent Actions: A white agent in the MiniWoB environment can perform fundamental browser-level actions that simulate typical human interactions on web pages. These include:
Clicking: Selecting buttons, links, or other clickable elements.
Typing: Entering text into input fields or forms.
Scrolling: Navigating vertically through the page to access hidden elements.
Submitting: Confirming an action when the task design requires it.
State Changes: Each action taken by the agent modifies the state of the page. For example:
Clicking a button may trigger a new element to appear or change color.
Scrolling over a list to visit the entire content or look through all possible choices.
Task Accomplishment: A MiniWoB task is considered accomplished when the agent performs the correct sequence of interactions that satisfy the instruction displayed at the top of the task window. Performance is measured by both accuracy (whether the agent completed the task correctly) and efficiency (how quickly and with how few steps it did so).

AssistantBench:
Goal of the Task: The primary goal for the agent is to answer a specific question posed by the user. These questions are designed to be complex, often requiring the agent to navigate to multiple websites, gather information, and synthesize it to arrive at a final answer.
State Space: The state space in AssistantBench is the entirety of the World Wide Web. At any given moment, the state is defined by the current web page the agent is on, the content of that page (including text, images, and interactive elements).
Agent Actions: A white agent in the AssistantBench environment can perform a variety of actions that mimic human web browsing behavior. These actions include:
Navigating to a URL: The agent can directly go to a specific web address.
Searching: The agent can use a search engine to find relevant web pages.
Clicking: The agent can click on links, buttons, and other interactive elements on a web page.
Typing: The agent can input text into search bars, forms, and other text fields.
Scrolling: The agent can scroll up and down a web page to view its entire content.
Returning to a previous page: The agent can go back to the previously visited page.
State Changes: The state of the environment changes based on the agent's actions. For instance:
Navigating to a new URL or clicking a link will change the current web page, presenting the agent with new information and a new set of possible actions.
Typing text into a search bar and submitting it will lead to a search results page, altering the state to display a list of links.
Task Accomplishment: A task is considered accomplished when the agent provides a correct and complete answer to the user's question. The task ends when the agent either submits an answer or reaches a predefined limit on the number of steps or time allowed for the task.
WebLINX 


WebLINX is designed as a structured, instruction-following web navigation benchmark. Compared to MiniWoB’s micro-tasks and AssistantBench’s open-web exploration, WebLINX focuses on multi-step reasoning across a controlled set of web pages. Each WebLINX task follows a predefined chain of pages from which the agent must extract information, combine results, and produce a final answer.

Goal of the Task:
The goal is to follow a sequence of navigation steps, gather information from each page, and assemble the final answer required by the task. Tasks typically require visiting 3–7 pages, identifying specific HTML elements, reading content, and synthesizing the extracted text.
State Space:
 The state at each timestep consists of:
The current webpage’s DOM structure
All visible text on the page
Scroll position
Available clickable elements (identified by uids)
Input fields (if present)
The agent's navigation history (previous URLs visited)
Any partial information the agent has extracted so far


Agent Actions:
 A White Agent in WebLINX can perform:
goto(url): navigate to the exact URL provided in the task
click(uid=...): click specific links or structured elements
scroll(x, y) — Scroll the page to reveal hidden elements.
extract(selector): read data from paragraphs, headers, tables, lists
text_input(text=...) — Type into a text field.
hover(uid=...) — Move the cursor to an element.
key_press(key) — Press a keyboard key (e.g., "Enter").
submit(answer=...) — Submit the final task output.
These actions approximate realistic web interaction without requiring raw CSS selectors.
State Changes:
click: loads a new page or expands an element.
scroll:updates what elements are visible.
text_input: changes the contents of form fields.
goto:replaces the entire DOM with a new page.
hover: updates the active, focused element.
submit: ends the task.
Task Accomplishment:
 The task ends when:
The agent submits its final answer


The max number of steps is reached


The agent leaves the required navigation chain (invalid trajectory)
The task is successful only if the submitted answer exactly matches or closely matches the ground-truth extracted from the correct chain of webpages.



Q7.2 Evaluation Design
Describe how your green agent evaluates white agents. For example, what are the metrics used to measure the white agent performance? How is the score for each metric evaluated? Please also provide a few examples of different trajectories taken by a white agent and how the green agent evaluates them.

Browsergym includes multiple different environments in one big package. We have implemented the following environments: MiniWoB,WebLINX and AssistantBench.

MiniWoB
Each web task has a reward function. For every action the agent takes, it will output a description of the current environment after the action and the corresponding reward. Our green agent records the reward of each action taken by white agent in the reward_history list and calculates the final average in the evaluate_task_completion() function.
-inputs: Basic battle information set up by the agentbeats platform and actions responded by the white agent.
-outputs: Summary of the process and final result report.
-success criteria: Whether the reward of the white agent's action is larger than the threshold.

Here's a short example of green agent evaluation output in one MiniWoB demo task:
[AgentBeatsExecutor] Agent output: The evaluation is complete. Here’s a summary of the process:

1. **Battle Initialized:**
   - **Battle ID:** `c53751e7-399e-4aa0-a151-e4226276f67a`
   - **White Agent URL:** `http://localhost:9111`

2. **Environment Reset:**
   - MiniWoB task environment reset successfully.

3. **Task Description Obtained:**
   - **Goal:** Select Natalya from the scroll list and click Submit.
   - **Task URL:** [Click Scroll List](http://localhost:8089/miniwob/click-scroll-list.html)

4. **White Agent's Actions:**
   - Actions given:
     ```plaintext
     page.goto("http://localhost:8089/miniwob/click-scroll-list.html")
     page.locator("text=Natalya").scroll_into_view_if_needed().click()
     page.get_by_role("button", name="Submit").click()
     ```

5. **Execution Result:**
   - Action executed with a reward of **0.0**.
   - **Error:** Timeout occurred while trying to scroll to the element.

6. **Task Evaluation:**
   - **Elapsed Time:** 33.9 seconds
   - **Last Action:** Attempt to click Natalie.
   - **Error Description:** TimeoutError.

7. **Final Report:**
   - The white agent failed to complete the task due to a timeout error.
   - **Winner:** **Draw**

All details have been logged and reported successfully. If you need further assistance or another evaluation, let me know!

AssistantBench
The evaluation is centered on the final answer provided by the agent and employs several key metrics.
The primary metric is Accuracy, which measures how closely the agent's answer matches the ground truth. For numerical answers, partial credit is given for close responses, while text-based answers are judged using an F1 score. Other metrics include:
Answer Rate: The percentage of tasks for which the agent provides any response.
Precision: The average accuracy calculated only for the tasks where an answer was given.
Exact Match: A strict measure that requires the agent's answer to be identical to the ground truth.
Scoring is nuanced: numerical answers are scored based on the order of magnitude of their difference from the correct answer, and text-based answers are scored on word overlap (F1 score). For structured data like JSON, the evaluation compares values of corresponding keys.
For example, a successful agent tasked with finding early morning fitness classes near a specific location would search for gyms, check their schedules online, and compile an accurate list. Conversely, an unsuccessful agent might get caught in a loop while navigating websites or provide a hallucinated answer from an unreliable source, both resulting in a low accuracy score or error.

WebLINX 
The green agent evaluates WebLINX trajectories using fine-grained, multi-dimensional scoring. Because tasks require multi-page reasoning, the evaluation considers both the process and the final result.
Metrics Used:
Exact Match (Final Answer)
 Full credit if the submitted answer matches the ground truth exactly.


F1 Score for Extracted Text
 Used for longer answers, multi-value extractions, or paragraphs.
 Provides partial credit for partially-correct answers.


Trajectory Correctness
 Measures whether the agent visited the required chain of pages.


1.0 if the agent follows the chain correctly
0.0 if it deviates into irrelevant pages


Action Efficiency
 Penalizes unnecessarily long trajectories such as repeated scrolling or clicking irrelevant items.


Evaluation Process:
 The green agent records:
all visited URLs
all extracted fields
the final submitted answer


Then it compares:
expected chain vs. actual chain
expected extracted fields vs. actual extraction
expected final answer vs. submitted answer


Example Trajectories:
Example 1 — Correct chain & correct answer
Agent follows URLs in correct order
Extracts all fields
Submits exact answer
 → Score: 1.0
Example 2 — Correct chain, partial extraction
Missed one required element


F1 = 0.6
 → Score: ~0.6
Example 3 — Wrong chain
Clicked unrelated link
 → Trajectory correctness = 0
 → Score: 0.0


7.3 Data Design
Describe the data you prepared, including the tasks and environments you prepared to test white agents in, and the test cases you prepared to test if the green agent can provide reasonable evaluation results on those test cases.
Browsergym includes multiple different environments in one big package. We have implemented the following environments: MiniWoB,WebLINX and AssistantBench.

MiniWoB

Data and Environments for “White Agents”:

MiniWoB (Mini Web-Based Tasks) provides a collection of 130 lightweight, atomic web tasks designed to rigorously test a white agent’s ability to interpret simple web interfaces, follow instructions, and execute fine-grained actions. These tasks span UI interactions such as clicking, typing, selecting, and navigating, offering a controlled environment ideal for evaluating low-level web manipulation competency.
In our setup, MiniWoB tasks are instantiated using BrowserGym, which standardizes environment creation and observation formatting. Each task environment returns structured state representations and rewards that allow the white agent to reason about the interface and decide subsequent actions.

Test Cases for “Green Agent” Evaluation:
To evaluate whether a green agent can reliably assess a white agent’s performance, MiniWoB tasks serve as clean, deterministic test cases. The green agent is responsible for instantiating each environment, providing the white agent with the task description (in JSON format), observing the white agent’s step-by-step actions, and computing task rewards.
BrowserGym’s API—gym.make(), env.reset(), and env.step()—ensures that all evaluations follow standardized execution semantics. Because MiniWoB tasks provide explicit success conditions and reward signals, the green agent can objectively determine whether the white agent’s actions complete the task correctly, making the suite ideal for validating evaluator accuracy.

AssistantBench
To better evaluate web agents, I have enhanced the AssistantBench benchmark by adding approximately 20 new test cases.
Data and Environments for "White Agents":
AssistantBench provides a robust testing ground with 214 tasks requiring agents to navigate over 525 pages on 258 websites. These tasks cover domains like e-commerce and travel and are designed to be complex. My contribution specifically introduced tasks that require German language proficiency and were created to reduce the original dataset's US-centric bias.
Test Cases for "Green Agent" Evaluation:
A "green agent" (evaluator) can use these test cases to gauge a "white agent's" performance. The test cases require answers in various formats like text, numbers, or JSON. Performance is measured by automatically comparing the agent's output to a pre-determined correct answer, allowing for an objective assessment of accuracy.


WebLINX 
Tasks Provided:
 Each task consists of:
A predefined list of 3–7 URLs


A description of fields that must be extracted from each page


The final question requiring synthesis of extracted data


Element uids for click targets


Expected final answers


Examples include:
Extracting product specifications from multiple tabs


Collecting academic metadata across author pages


Navigating Wikipedia-like pages and summarizing content


Reading tables that span multiple linked documents


Test Cases for Green Agent Validation:
 We created controlled trajectories to test evaluator correctness:
Perfect trajectory with exact answer → expected score 1.0


Correct chain but missing a field → expected F1 < 1


Incorrect link clicked → expected score 0


Extra steps but correct final answer → reduced efficiency score


Non-exact paraphrased answer → mid-range F1 score


These test cases ensure the green agent provides consistent, robust evaluations.





Q8.2 Faithfulness / Scope & Scale
Does your implementation of the benchmark reproduce the results from the original benchmark? How did you ensure this? Please also provide quantitative results that show your re-implementation of the benchmark can reproduce the results in the original benchmark. Note that you will also need to provide commands to reproduce these results in the GitHub repo you submit.
Browsergym includes multiple different environments in one big package. We have implemented the following environments: MiniWoB,WebLINX and AssistantBench.

MiniWoB

The implementation focuses on a faithful reproduction of the MiniWoB environments included in BrowserGym. To ensure this, we first ran the official BrowserGym MiniWoB tasks locally to understand their task configuration, HTML structures, and reward mechanisms. We then reproduced these environments inside AgentBeats through a new scenario, scenario4BrowserGym-miniwob, where the Green Agent maintains environment control and reward tracking while the White Agent interacts with the environment through well-defined tool interfaces.
To preserve fidelity, we performed several technical consistency checks:
Tool definitions: We added new methods in tools.py to wrap BrowserGym environment control and expose task-specific information (e.g., goals, observations) to the White Agent.
Configuration alignment: We updated the .toml files so the declared tools match the actual methods used during agent–environment interaction.
Environment behavior verification: By comparing the BrowserGym environment outputs (env.reset() and reward transitions from env.step(action)), we ensured they behave the same inside AgentBeats.
Following the last demo phase’s feedback, we additionally ran a simple baseline agent—one that selects random valid actions—on both (1) the official BrowserGym MiniWoB environment and (2) our AgentBeats-integrated version. Across all 130 MiniWoB tasks, the success rates differed a little, and the final score given by the web tasks are also close, confirming that our re-implementation preserves the logic and difficulty of the original benchmark.
This confirms that our MiniWoB environment accesses the same observations, rewards, and termination conditions as the official BrowserGym version.
AssistantBench
Our implementation focuses on a faithful reproduction of the AssistantBench evaluation logic. Directly replicating the original paper's end-to-end agent scores is currently infeasible. The AssistantBench tasks are exceptionally challenging by design—the state-of-the-art agent in the paper only achieved ~26% accuracy.
Consequently, our chatgpt-mini-based agent lacks the advanced reasoning required for these tasks, and it also frequently encounters reCAPTCHA challenges on the live web that block progress.
Given these constraints, we validated our Green Agent's scoring mechanism with a dedicated test suite. This suite confirms our implementation correctly applies the metrics from the paper's Section 3.4 for various answer types:
Exact Match: Correctly assigns full credit (1.0) for perfect answers.
Partial F1 Match: Accurately calculates partial credit for list/JSON answers.
Numerical Closeness: Faithfully applies the order of magnitude metric.
These tests verify that our scoring is accurate and reliable. To reproduce our validation of the evaluation logic, run:
pytest tests/test_evaluation_faithfulness.py
WebLINX 

To ensure that our WebLINX integration behaves consistently with the original BrowserGym benchmark, we conducted a series of targeted sanity checks using the official WebLINX evaluator.
Instead of attempting to reproduce the full benchmark results, we verified the correctness of the scoring logic by manually constructing three representative trajectories:
a fully correct chain,


an incorrect chain,


and a partially correct extraction.


These cases directly correspond to the core dimensions evaluated in WebLINX:
whether the agent follows the correct navigation chain, and


whether the final extracted answer matches the ground truth.


For each trajectory, we compared the evaluator’s output with the expected behavior described in the BrowserGym documentation (e.g., ~1.0 for a correct chain, 0.0 for an incorrect chain, and an intermediate score for partially correct extraction).
 The outputs aligned with these expectations, confirming that the built-in evaluator works as intended and that our WebLINX setup is faithful to the original benchmark design.











Question 8.3 Coverage Expansion / Realism
Does your implementation of the benchmark expand the coverage of the original benchmark (e.g., broader coverage of more tasks, more test cases, etc.)? Describe in detail how you expand the coverage.
Browsergym includes multiple different environments in one big package. We have implemented the following environments: MiniWoB,WebLINX and AssistantBench.

MiniWoB
Yes, in our implementation, we expanded the coverage and realism of the original MiniWoB benchmark by extending several of its existing HTML-based tasks into more complex variants.
For example, the original choose-list task involves selecting a single item from a simple list. We expanded this into a choose-multi-list task that contains multiple scrollable lists nested in different layers, requiring the agent to perform multi-level scrolling and multi-step selection properly. We applied similar extensions to other MiniWoB tasks, adding additional elements, more complex widget hierarchies, or multiple sequential subtasks. These modifications preserve the spirit of the original MiniWoB tasks while increasing their difficulty and realism, offering a broader stress test for fine-grained action planning and UI manipulation abilities.
In short, our enhancements increase coverage by introducing new task variants grounded in the original HTML templates, providing a more comprehensive and challenging MiniWoB suite.
AssistantBench
Yes, in our implementation, we expanded the benchmark's coverage and realism by adding new questions in tool.py that increase its geographic diversity.
The original benchmark was primarily US, with tasks focused on American cities, stores, and websites.
Our expansion introduces European focused tasks.
This change enhances the benchmark's realism by forcing agents to navigate unfamiliar, region-specific websites (e.g., .de domains) and handle different contexts, such as prices in Euros. In short, our work tests if an agent can serve a global user, not just an American one, making it a more robust measure of an agent's real-world capability.

WebLINX 

No, we did not expand the coverage of the original WebLINX benchmark.
 Instead, our implementation focuses on faithfully reproducing the original WebLINX behavior and ensuring that all tasks remain functional within BrowserGym.
WebLINX relies on real-world websites, which may change over time.
 Therefore, our primary contribution was not adding new task types, but:
Verifying the correctness of the evaluator through multiple sanity tests


Ensuring that all URLs used in the existing tasks are functional


Checking that each task still supports faithful chain navigation and correct element extraction


These steps improve the stability and reproducibility of the original WebLINX benchmark without altering its task distribution.
 In short, while we did not expand WebLINX’s coverage, our work ensures a reliable and accurate reproduction of the benchmark in the BrowserGym environment.

Q8.4 Evaluator Quality
Is the metric faithfully reflecting the capability of white agents in the task? Is the metric fine-grained enough to discriminate different white agents (e.g., instead of only evaluating the final outcome, we should also evaluate the process)? Is the evaluation consistent across different runs? Describe how you ensure these. Please also provide evaluation results of different white agents you tried, analyze if their performance are different, and provide some examples to explain what difference in their capabilities has caused this difference in performance.

Browsergym includes multiple different environments in one big package. We have implemented the following environments: MiniWoB,WebLINX and AssistantBench.

MiniWoB
In our MiniWoB setup, evaluator quality is ensured by keeping all scoring logic inside the Green Agent, which directly reads the environment’s observations, rewards, and termination signals. This removes ambiguity from agent-generated descriptions and guarantees consistent evaluation across runs. The Green Agent applies the same reward thresholds and completion rules defined in the original BrowserGym benchmark, ensuring faithful measurement of task success.
To verify that the metric can distinguish different White Agents, we tested multiple agent variants. A simple baseline agent that issues random actions consistently receives low rewards, while a more capable agent that interprets the task description and responds to environment feedback performs better. These performance gaps align with clear behavioral differences (e.g., correct multi-step navigation vs. incorrect clicks), showing that the metric is fine-grained enough to reflect true capability differences.

AssistantBench
AssistantBench is a benchmark designed to test AI agents on complex, realistic tasks using a live web browser. Its purpose is to see if an AI can effectively find and synthesize information like a human would, often across multiple websites.
The benchmark's strength lies in its high-quality evaluation system. It scores agents fairly with partial credit, provides detailed error analysis to show why an agent failed (e.g., getting lost or clicking the wrong thing), and successfully identifies which agents use smarter strategies.
However, the evaluation has a key limitation: it relies on matching exact words rather than understanding the meaning. This means a perfectly correct but paraphrased answer can be unfairly scored as zero, potentially penalizing more advanced agents that demonstrate true comprehension.
Overall, AssistantBench is a valuable tool for advancing web-based AI, pushing them to solve real-world problems, even if its scoring doesn't always capture the full extent of an agent's intelligence.
WebLINX 
In our WebLINX integration, evaluation quality is ensured by using the official BrowserGym WebLINX evaluator, which provides a deterministic and fully specified scoring mechanism. The metric considers two key aspects: whether the agent followed the correct navigation chain, and whether the final extracted answer matches the ground truth (with partial credit for incomplete or partially correct outputs).
To verify that the metric is fine-grained and can distinguish agent capabilities, we evaluated several representative trajectories, including a fully correct chain, an incorrect chain, and a partially correct extraction. These trajectories produced clearly different scores (≈1.0, 0.0, and intermediate values), demonstrating that the evaluator reliably captures meaningful differences in behavior and output quality.
Because the scoring depends only on the recorded interaction trajectory, the evaluation is deterministic and produces consistent results across repeated runs. This ensures that the WebLINX metric faithfully reflects the agent’s navigation and extraction performance without ambiguity.

Q8.5 Validation

Did you conduct manual or spot validations on the Green Agent outputs to ensure the evaluation results are accurate? How did you conduct the validations? Please provide 3 examples of different test cases and the evaluation results the green agent outputs for these test cases. Note that you will need to provide command that reproduce the evaluation results on your test cases in the GitHub repo you submit.

Browsergym includes multiple different environments in one big package. We have implemented the following environments: MiniWoB,WebLINX and AssistantBench.

MiniWoB
We conduct regular spot checks by manually inspecting the environment’s rendered HTML state and corresponding Green Agent logs.
For example, we compare recorded obs["goal"], info["score"], and the DOM structure snapshots against the ground-truth MiniWoB web pages to ensure correct perception and scoring.
Additionally, we replay stored interaction traces to confirm that the Green Agent’s evaluate operations align with actual task outcomes (e.g., correct button clicked, form filled, or list item selected).
AssistantBench
Yes, manual validation was performed by spot-checking random test runs. A single command to reproduce these specific outcomes is not feasible, as the agent's trajectory is non-deterministic due to the live web environment, random task selection, and the stochastic nature of LLMs.
Here are two validation examples from these spot-checks:
Agent Looping: An agent got stuck in a loop and failed the task. The evaluator correctly assigned a score of 0, which was validated as the right assessment.
Web Blocking: An agent was blocked by a reCAPTCHA during a search and failed. It was also correctly scored 0, as the evaluator bases its score on successful task completion, even if the failure is due to environmental challenges.
WebLINX 
For WebLINX, we performed targeted manual validations against the official BrowserGym evaluator. Because WebLINX scoring is deterministic and based solely on the recorded trajectory, these validations can be reproduced consistently. To ensure evaluator correctness, we constructed controlled trajectories and manually inspected the scoring outputs. This allowed us to confirm that the Green Agent and the built-in evaluator produced identical results for the same trajectory. Three representative validation cases are as follows.
First, in a fully correct chain case, the agent visited every required URL in the correct order, extracted all necessary fields, and submitted the exact ground-truth answer. The evaluator assigned a score of 1.0.
Second, in a partial extraction case, the agent followed the correct URL chain but missed one required field during extraction. The evaluator produced an intermediate F1-based score below 1.0, reflecting partially correct output.
Third, in an incorrect chain case, the agent clicked a wrong link and navigated to a page outside the required chain. The evaluator assigned a score of 0, demonstrating correct handling of invalid trajectories.



Question 8.6 Robustness
Do your evaluation scripts and Green Agents run robustly on AgentBeats? How did you ensure that?
Browsergym includes multiple different environments in one big package. We have implemented the following environments: MiniWoB,WebLINX and AssistantBench.

MiniWoB
Our Green Agent and evaluation scripts are designed to be robust under the AgentBeats runtime through modularization. Each task environment (e.g., MiniWoB HTML) runs inside an independent Gym instance with controlled reset and termination logic, ensuring that a task failure does not affect subsequent runs.
We added retry and timeout mechanisms for environment initialization and tool invocation, preventing async/sync deadlocks that previously occurred due to BrowserGym’s greenlet model.
AssistantBench
Ensuring the robustness of a benchmark that runs on the live web is a significant challenge. The primary issues are the dynamic nature of websites and the prevalence of anti-bot measures which can cause even a capable agent to fail.
To ensure our evaluation scripts and Green Agents run robustly on AgentBeats, we have implemented several key features:
reCAPTCHA Detection: We implemented a "reCAPTCHA catcher" that actively monitors for anti-bot pages. Since current agents cannot solve these, this mechanism gracefully terminates the battle when a reCAPTCHA is detected. This prevents agents from getting stuck in unrecoverable states and ensures the evaluation harness does not hang.
General Error Handling: The scripts include try-except blocks to resiliently handle common web automation errors, such as page load timeouts or elements not being found.
Standardized Tools: We utilize the stable and well-maintained Playwright library for browser automation, which is consistent with the original benchmark's design.

WebLINX 
Yes. We performed manual spot validation of the WebLINX evaluator outputs to ensure that the scoring behavior matched the expected outcomes defined by the official BrowserGym evaluator.
 Because WebLINX uses deterministic scoring based solely on the recorded trajectory, these validations can be reproduced exactly using the commands included in our repository.
To validate correctness, we constructed three representative trajectories: a fully correct chain, an incorrect chain, and a partially correct extraction. For each case, we manually inspected the sequence of actions, the extracted content, and the final score produced by the evaluator to confirm that it aligned with the WebLINX scoring design.



Question 8.7 Bias or Contamination
Did you ensure if the evaluation is biased or contaminated? How did you ensure there is not bias or contamination? Provide quantitative analysis of whether your benchmark is biased or contaminated and whether you’ve fixed the problem.

Browsergym includes multiple different environments in one big package. We have implemented the following environments: MiniWoB,WebLINX and AssistantBench.

MiniWoB
To prevent bias, the Green Agent never exposes hidden task solutions or internal reward logic to the White Agent.
All task descriptions are restricted to observable HTML content, DOM trees, and textual goals—matching what a human or model would legitimately see. White Agents only interact with runtime environments through the communication with the green agent and defined tool APIs, without access to evaluation scripts.
AssistantBench
Bias: Our analysis reveals potential biases in the benchmark. The task sources favor a technologically proficient demographic, potentially not reflecting the needs of a broader global population. Furthermore, the benchmark's strong focus on English-language, US-centric tasks limits its ability to predict agent performance in multilingual or other international contexts.
Contamination: Regarding data contamination, our review found no evidence of quantitative checks (e.g., n-gram overlap) to verify that tasks were absent from model training data. The absence of this validation means we cannot definitively separate successful task completion from memorization. Therefore, the possibility of contamination must be considered when interpreting the results. We have added our own question, that is unique, which avoids contamination.
WebLINX 

For WebLINX, we did not find any meaningful sources of bias or contamination.
 The evaluator only looks at the agent’s actual browser actions (navigation and extraction), so it does not depend on language, prior knowledge, or demographic factors.
To ensure there was no leakage or contamination:
White Agents never receive ground-truth answers or evaluation logic. They only see what is on the rendered webpage.


All tasks use public websites, and our implementation stays close to the original WebLINX format. Since the agents are not trained on these tasks, there is no path for training-time contamination.


The WebLINX evaluator always scores based on the trajectory itself, and our sanity tests showed it behaves consistently across different types of trajectories.


Overall, we did not observe any bias or contamination issues in our WebLINX setup, and no additional mitigation was needed.



Question 8.8 Impact
Is the implementation reusable, well-documented, and presented clearly? How did you ensure that? Note that you will need to have a README of the documentation in your GitHub repo.




#####DO THE README####
