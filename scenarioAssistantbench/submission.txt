Q6 Abstract
Briefly describe the task that your white agent is tested on.

Our white agents are designed to solve tasks across three BrowserGym environments: MiniWoB, AssistantBench, and WebLINX. In MiniWoB, it measures an agent’s ability to complete fine-grained HTML interaction tasks using environment rewards. In AssistantBench, it evaluates multi-website information-seeking tasks using accuracy, F1, and exact-match metrics. In WebLINX, it assesses multi-step web navigation by scoring chain correctness and extraction quality. Together, the BrowserGym environment provides a unified and reliable evaluation of web-based reasoning and interaction skills.


Q7.1 Agent Framework Design
Describe the architecture and overall decision-making framework of your white agent. 
What is the whole decision making pipeline? 
What input does the white agent take at each step and what is the output? 
What modules does your agent have (planner, executor, memory, verifier, tools, reflection, etc.)? How do they interact? 
Does the agent use chain-of-thought, tool-augmented reasoning, multi-step planning, action refinement, program synthesis, etc.?

MiniWoB

The white agent of miniwob takes inputs from the green agent, which include basic BrowserGym-MiniWoB environment information, such as the url of the html task, the basic description of it and related messages. Then it perform a task-solving pipeline as
 “0. Process the information given by the green agent, understand the current web environment and the task goal you need to finish
1. Generate a Playwright action to complete the task
2. Call the functions in your tool to complete the task and get action description from the return value.
3. Return Playwright action(s) decription to the green agent. The green agent will evaluate your performance by calculating the reward of your action. You need to perform as well as you can.
4. Call the retry tool to try multiple execution of the actions if the first action failed.
5. Log the action using update_battle_process(battle_id, message, reported_by, detail, markdown_content) tool."
”, with tool-augmented process, which was supported by an extension tool file that  provides functions for it.

AssistantBench:

The white agent operates within the AssistantBench green-agent loop. After reset_assistantbench_env(), 
the green agent sends observations (goal, URL, AXTree) via talk_to_agent(...). 
The white agent uses the architecture in white_agent_card.toml, consisting of working memory, a short-horizon planner, 
an executor that validates AXTree targets, a verifier for redirects/blockers, and a reflection module for recovery.

Each turn, it runs a fixed chain-of-thought template (situation → memory → plan → reasoning) and outputs one action from the allowed set (goto, fill, click, press, send_msg_to_user). 
It plans multi-step navigation but executes actions singly. The green agent then executes the action and returns the next observation.

WebLINX 



Q7.2 Data & Evaluation Design for the White Agent
Describe the tasks and data that are used to test the white agent.
Describe the evaluation metrics of the tasks.
Describe the prompts used for the white agent.
Describe some main results / performance on the white agent on the benchmark


MiniWoB

MiniWoB (Mini Web-Based Tasks) provides 130 lightweight and atomic web tasks designed to evaluate a white agent’s ability to understand simple web interfaces, follow textual instructions, and perform precise UI actions such as clicking, selecting, typing, and scrolling. These tasks create a controlled environment ideal for testing low-level interface interaction skills.
In our setup, MiniWoB tasks are instantiated through BrowserGym, which standardizes the environment’s observations and reward structures. Each MiniWoB environment returns structured states and rewards, enabling the white agent to decide actions step by step.
For evaluating the green agent, MiniWoB serves as a deterministic test bed. The green agent instantiates each task, gives the task description to the white agent, monitors the sequence of actions, and computes rewards to assess correctness. Because MiniWoB tasks include explicit success rules and reward signals, they are particularly useful for verifying whether the green agent evaluates white agent behaviors accurately.
The white agent can perform valid actions and return action information log to the green agent to check. It achieves a relatively good reward mark(reach more than 0.5, whole reward range from 0~1) most of the time if the time limit is long enough for the actions to complete. However, it can not survive the original timeout set. Because the communication between the two agents takes more time than the original benchmark’s setting. 


AssistantBench:


AssistanceBench covers question answering plus  our custom Europe-focused tasks (often in non-English languages).
Evaluation focuses on the final answer, using Accuracy and auxiliary metrics: numerical responses receive partial credit by order-of-magnitude closeness, text answers use word-overlap F1, and structured JSON is scored by matching corresponding keys.
The white agent uses a structured prompt format—plan + single action per turn, with required tool syntax. It must search first, cite sources, and handle cookies, paywalls, and redirects before acting.

Results vary because the underlying Playwright stack frequently triggers Google’s bot protection (reCAPTCHA), which the agent cannot solve. 
When this happens, it gets stuck and retries different browser options, but most attempts encounter the same issue.
For demonstrations, we enabled a non-headless browser so a human can manually solve reCAPTCHA, allowing the agent to complete more tasks. Still, some sites block access even after human verification.

Task difficulty varies, and without reCAPTCHA barriers the white agent succeeds on roughly one-third of tasks, slightly above the rate reported in the AssistanceBench paper.
For battle submissions the agent runs headless, so recaptchas cannot be solved; if one is detected, the episode terminates early to save compute. If not, the agent proceeds to attempt the task.



WebLINX 

Question 8.1 Performance Improvement Over the Existing Baselines
Q8.1 Performance Improvement Over the Existing Baselines
What are the existing baselines or baselines you designed?
How much better is your white agent than the existing baselines?
What are the key designs/factors that make your white agent better than the baselines?

MiniWoB

The existing baseline we used in the green agent phase include three different levels. First we implemented a dummy white agent which simply return a textual response instead of a specific valid action. This is designed to test the agents basic structure and are not quite useful for its true purpose. Second baseline white agent is called “button-push” agent. This agent will complete the task in the simplest but fastest way by clicking the submit button. We design this agent so that we can test the least time we need for the white agent to perform a valid action as a efficiency baseline. The third baseline white agent is more functional and can return basic valid actions.
Our current white agent is improved from the 3rd version of our baseline. Specifically we equipped the white agent with more function tools to do better reasoning and more complex actions. Furthermore, we added retry mechanism for the white agent to try multiple times when their first actions failed. In our original baseline, the white agent only try solving the problem once and if the action is not valid it won’t try a second time and just wait for the task to end due to timeout.  We also further modify the prompt for the white agent so that it can perform a step-by-step task solving process.   

AssistantBench:

We compare against three baselines:

A naive text-only responder that never navigates (reward ≈ 0).

A one-shot “answer-now” agent with no planning or page handling, which fails whenever the answer is not already known.

A basic white agent that can navigate but lacks structured planning, validation, and task-specific logic.

Performance improvement.
The shipped white agent substantially outperforms all baselines. It reliably completes multi-step tasks that baselines (1) and (2) almost always fail, and it solves tasks faster and with far fewer errors than baseline (3). Overall success rate and efficiency are materially higher, especially on tasks requiring navigation, retries, or dynamic pages.

Key factors.
The gains come from: a reset-first control loop, explicit structured planning, single-action execution, and guards for cookies/paywalls/redirects. Action validation with retry and reflection prevents cascading failures. Together, these designs improve robustness and efficiency beyond the basic navigating agent.


WebLINX 





Q8.2 Generalizability to Different Test Scenarios
Does your agent generalize beyond the specific tasks it was tuned or designed for?
Provide results from unseen tasks or held-out environment configurations, etc.

MiniWoB

In our green agent phase we implemented more html tasks for the existing MiniWoB benchmarks so that it contains more complex problems. For example we implemented a choose-multi-list task based on the choose-list task. And the white agent can also solve these extended benchmark tasks.

Take the choose-multi-list task as example, the original task configuration requires the white agent to scroll a choice list and push a submit button after choosing the required bar. In the choose-multi-list task, we implemented a 2-layer choice list and the white agent needs to choose the first choice so that the second list will show on the panel. Then it must choose the second choice to finish the task. And as we mentioned before it can reach a relatively good reward mark.  

AssistantBench:

The agent is designed to generalize across AssistantBench tasks, which are diverse and difficult to overfit. 
It operates only on the provided goal, URL, and AXTree, using search-first heuristics and generic tool-selection rules. 
The green agent can sample any VALID_AB_TASK_IDS, and our custom task file adds 16 open-web questions evaluated with the same scorer or string matching. 
No task-specific templates are used beyond shared chain-of-thought and web-handling protocols. 
Spot checks on unseen custom tasks show the agent follows the same Google-first → click → extract pattern without prompt changes, with performance primarily limited by web availability and blockers.


WebLINX 


Q8.3 Reasoning Quality / Interpretability
Does your agent produce coherent, structured, and interpretable reasoning aligned with the task? Whether reasoning steps follow logically from observations.
Provide 2–3 example trajectories showing high-quality reasoning and 1 trajectory showing failure + analysis.


MiniWoB

The white agent shows that it can generate coherent and consistent reasoning aligned with the html tasks. It follows a step-by-step TaskAnalyze->ProbelmSolve->GenerateActions process to solve most of the html tasks.  Most of the time the white agent can perform valid actions as it first navigates to the url link, and then performs valid actions like clicking, scrolling, choosing, and finally submits the task. Sometimes it fails to generate the final push submit button action and thus fails. Or sometimes when the task is more complicated and requires a series of actions, it will get stuck after some middle actions. We think that’s because the agent can’t gain full insight of the whole task structure as the html becomes more complex.   


AssistantBench:

The agent produces coherent, structured, and interpretable reasoning that closely aligns with task objectives. Observations, plans, and actions follow a clear and logical progression.
Successful trajectories (examples):
Search-based entity lookup.
Trajectory: reset → observe Google homepage → plan search → goto('https://www.google.com') → fill(searchbox_bid, 'Forbes 30 Under 30 2026 AI Decagon') → press('Enter') → click Forbes result → navigate to AI category → open Jesse Zhang profile → send_msg_to_user('Jesse Zhang').
UI interruption handling.
When encountering a cookie banner, the agent correctly prioritizes “Accept All” before continuing the search, adapting without losing task focus.
Failure trajectory (with analysis):
CAPTCHA blocking.
The agent detects reCAPTCHA via AXTree signals and requests manual solving. After proceeding, it encounters additional CAPTCHA blocks on TripAdvisor that persist despite human intervention.
Analysis: The failure is caused by external platform restrictions, not by flawed reasoning; detection and escalation behavior remain correct and interpretable.


WebLINX 





Q8.4 Efficiency & Resource Use
Does the white agent solve tasks efficiently? Measure this by the number of steps or actions taken, token/compute efficiency, etc.

MiniWoB

The efficiency is normal but not very good. From the whole process’s perspective, the time it takes to finish the task is a little bit long. In the above questions we mentioned the timeout problem and that’s hard to solve. Even with the simplest baseline white agent, the reaction and communication still takes some time. However, as for the token and compute process, the white agent seems efficient enough as it can understand most of the task quickly and generate valid actions in a few tokens. 

AssistantBench:

The white agent is generally efficient: simple tasks are solved in few, direct steps with low token and action usage. Efficiency is bounded because each atomic action (e.g., click, type, navigate) requires a separate model call, steps are not batched or combined. More complex tasks may exceed the 30-step limit if the initial approach is suboptimal. To avoid wasted computation, we implement early termination when progress is blocked (e.g., reCAPTCHA or hard paywalls), preventing repeated searches or browser retries.


WebLINX 


Q8.5 Bias, Overfitting, or Contamination Checks
Did you ensure that your white agent is not overfitting to specific test cases or leaking benchmark answers?
If using external tools/data, demonstrate that answers are not leaked or contaminated

MiniWoB

The white agent is not overfitting to specific test cases in the whole MiniWoB benchmark. The access of data and tasks are controlled by the green agent. Basically, the white agent can only receive the html task’s information from the green agent, which include the url of the html task, the basic description of it and other task-related messages. There’s no leaking or contamination in the tools or data. The tool augmentation of the white agent only supports it to better performance for general html tasks and does not contain any task-related information that will probably leak other data to it.

AssistantBench:

We took explicit steps to prevent overfitting and answer leakage. The agent can only use live web pages accessed through the BrowserGym environment; it has no access to static datasets, benchmark labels, or cached answers. For the default task, the gold answer is defined in code and evaluated via exact string match; all other tasks are scored by the AssistantBench evaluator against held-out references.
The prompt enforces single-action execution (no multi-step dumps), reducing accidental leakage. No prompt-tuning or adaptation was performed on evaluation items—the same generic reasoning and web-interaction protocol is reused across all tasks.
While any general-purpose LLM has a non-zero chance of prior exposure, most AssistantBench and our custom tasks are sufficiently unique, and our dataset is newly constructed, making memorization or contamination extremely unlikely.


WebLINX 



Q8.6 Impact, Reusability, and Documentation Quality

Is your white agent implementation reusable, modular, and easy for others to understand?
Provide clear README, instructions, and runnable examples in the GitHub repository submission.

Yes, the white agent is defined clearly and easy to be reused. Our GitHub Link is attached:  
https://github.com/zzzoer/agentbeats-browsergym



The report should be 1-2 pages long (a nice overleaf latex report, if required with citations)

Abstract section: a brief overview/abstract of the important information of the whole report (e.g., what the task is, what the metric is, how the white agent is designed, what the main quantitative results are, how its performance is compared to baselines, etc.). Keep it brief by using 1-2 sentences for each bullet point.

Benchmark section: A detailed introduction of the benchmark. This should include a brief overview of the literature and related work on previous benchmarks on the same/similar task, an overview of the benchmark you are using (what the benchmark evaluates, what the metric is, what kind of tasks or data are there in the benchmark, etc.), and details about the benchmark (e.g., what the inputs and outputs are for a white agent at each step, how the green agent evaluates a white agent, etc.)

White agent framework section: A detailed description of the framework. It should cover all the points in Question 7(a).

Experiment section: A detailed description of all the qualitative/quantitative results on the white agent. It should cover all the points in Question 8.

The most important attached document is white_agent submission.pdf (the whole report should be based around that)

Read following websites and papers as well and cite them, if requires:


I have attached following Websites:

Browsergym
https://github.com/ServiceNow/BrowserGym


AssistanceBench Website:
https://github.com/oriyor/assistantbench
https://assistantbench.github.io/


Miniwob Website:
https://github.com/Farama-Foundation/miniwob-plusplus
https://miniwob.farama.org/


WebLINX
https://github.com/McGill-NLP/weblinx
https://mcgill-nlp.github.io/weblinx/








Your video should demonstrate the framework of your white agent, how your white agent completes the tasks, and its evaluation results.

Your demo must include:
1. Task Introduction
2. What is the task?
3. What does the environment look like?
4. What actions can each agent take?

Agent Framework
1. What is the overall framework design of the white agent?
2. What is the decision making pipeline of the white agent?
3. What are the inputs and outputs of the agent at each step?

Demonstration
1. Show how your white agent completes different tasks on a few tasks.
2. Clearly explain the input and output of the white agent at each step, and explain why the agent takes each action.
3. Display the quantitative result of white agents on the benchmark (e.g., the accuracy).

Tips:
1. Keep it concise— 5 minutes max.
2. You may record a screen-share walkthrough or an edited short demo.
3. Audio narration or captions are encouraged for clarity.